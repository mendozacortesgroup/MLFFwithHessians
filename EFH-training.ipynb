{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Train Neural Network Potential To Energies, Forces, and Hessians\n",
    "==========================================================\n",
    "\n",
    "This tutorial shows how to train a Neural Network Potential based on ANI-1 to energies, forces, and Hessians.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self atomic energies:  tensor([ -0.6138, -38.0539, -54.7216, -75.1908])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchani\n",
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility (seed: 7289038)\n",
    "random.seed(7289038)\n",
    "np.random.seed(7289038)\n",
    "torch.manual_seed(7289038)\n",
    "\n",
    "# Set the default dtype for tensors\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# helper function to convert energy unit from Hartree to kcal/mol\n",
    "from torchani.units import hartree2kcalmol\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "\n",
    "Rcr = 5.2000e+00\n",
    "Rca = 3.5000e+00\n",
    "EtaR = torch.tensor([1.6000000e+01], device=device)\n",
    "ShfR = torch.tensor([9.0000000e-01, 1.1687500e+00, 1.4375000e+00, 1.7062500e+00, 1.9750000e+00, 2.2437500e+00, 2.5125000e+00, 2.7812500e+00, 3.0500000e+00, 3.3187500e+00, 3.5875000e+00, 3.8562500e+00, 4.1250000e+00, 4.3937500e+00, 4.6625000e+00, 4.9312500e+00], device=device)\n",
    "Zeta = torch.tensor([3.2000000e+01], device=device)\n",
    "ShfZ = torch.tensor([1.9634954e-01, 5.8904862e-01, 9.8174770e-01, 1.3744468e+00, 1.7671459e+00, 2.1598449e+00, 2.5525440e+00, 2.9452431e+00], device=device)\n",
    "EtaA = torch.tensor([8.0000000e+00], device=device)\n",
    "ShfA = torch.tensor([9.0000000e-01, 1.5500000e+00, 2.2000000e+00, 2.8500000e+00], device=device)\n",
    "species_order = ['H', 'C', 'N', 'O']\n",
    "num_species = len(species_order)\n",
    "aev_computer = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA, Zeta, ShfA, ShfZ, num_species)\n",
    "energy_shifter = torchani.utils.EnergyShifter(None)\n",
    "\n",
    "\n",
    "try:\n",
    "    path = os.path.dirname(os.path.realpath(__file__))\n",
    "except NameError:\n",
    "    path = os.getcwd()\n",
    "#dspath = os.path.join(path, '../dataset/ai-1x/sample.h5') # <- Original line\n",
    "dspath = os.path.join(path, './molecules-RTP.h5')\n",
    "\n",
    "config = {\"max_epochs\":5000, \"batch_size\":400, \"hidden_layer_sizes\":[256,64,256]}\n",
    "\n",
    "# Change the proportions of the training, validation, and test sets as needed\n",
    "training, skip, validation, test = torchani.data.load(dspath, additional_properties=('forces','hessian')\n",
    "    ).subtract_self_energies(energy_shifter, species_order).species_to_indices(species_order).shuffle().split(0.8, 0, 0.1, None)\n",
    "\n",
    "training = training.collate(config[\"batch_size\"]).cache()\n",
    "validation = validation.collate(config[\"batch_size\"]).cache()\n",
    "test = test.collate(config[\"batch_size\"]).cache()\n",
    "\n",
    "print('Self atomic energies: ', energy_shifter.self_energies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to define networks, optimizers, are mostly the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANIModel(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
      "    (1): CELU(alpha=0.1)\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (3): CELU(alpha=0.1)\n",
      "    (4): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (5): CELU(alpha=0.1)\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
      "    (1): CELU(alpha=0.1)\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (3): CELU(alpha=0.1)\n",
      "    (4): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (5): CELU(alpha=0.1)\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
      "    (1): CELU(alpha=0.1)\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (3): CELU(alpha=0.1)\n",
      "    (4): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (5): CELU(alpha=0.1)\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
      "    (1): CELU(alpha=0.1)\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (3): CELU(alpha=0.1)\n",
      "    (4): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (5): CELU(alpha=0.1)\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "aev_dim = aev_computer.aev_length\n",
    "\n",
    "H_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 256),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(64, 256),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(256, 1),\n",
    ")\n",
    "\n",
    "C_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 256),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(64, 256),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(256, 1),\n",
    ")\n",
    "\n",
    "N_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 256),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(64, 256),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(256, 1),\n",
    ")\n",
    "\n",
    "O_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 256),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(64, 256),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(256, 1),\n",
    ")\n",
    "\n",
    "nn = torchani.ANIModel([H_network, C_network, N_network, O_network])\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights and biases.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Pytorch default initialization for the weights and biases in linear layers\n",
    "  is Kaiming uniform. See: `TORCH.NN.MODULES.LINEAR`_\n",
    "  We initialize the weights similarly but from the normal distribution.\n",
    "  The biases were initialized to zero.</p></div>\n",
    "\n",
    "  https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANIModel(\n",
       "  (0): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (1): CELU(alpha=0.1)\n",
       "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (3): CELU(alpha=0.1)\n",
       "    (4): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (5): CELU(alpha=0.1)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_params(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "nn.apply(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a pipeline of AEV Computer --> Neural Networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchani.nn.Sequential(aev_computer, nn).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use multiple GPUs for the training, we can split the data between them with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Sequential(\n",
       "    (0): AEVComputer()\n",
       "    (1): ANIModel(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (1): CELU(alpha=0.1)\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): CELU(alpha=0.1)\n",
       "        (4): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (5): CELU(alpha=0.1)\n",
       "        (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (1): CELU(alpha=0.1)\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): CELU(alpha=0.1)\n",
       "        (4): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (5): CELU(alpha=0.1)\n",
       "        (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (1): CELU(alpha=0.1)\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): CELU(alpha=0.1)\n",
       "        (4): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (5): CELU(alpha=0.1)\n",
       "        (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "        (1): CELU(alpha=0.1)\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): CELU(alpha=0.1)\n",
       "        (4): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (5): CELU(alpha=0.1)\n",
       "        (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use Adam with weight decay for the weights and Stochastic Gradient\n",
    "Descent for biases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW = torch.optim.AdamW([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].weight]},\n",
    "    {'params': [H_network[2].weight]},\n",
    "    {'params': [H_network[4].weight]},\n",
    "    {'params': [H_network[6].weight]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].weight]},\n",
    "    {'params': [C_network[2].weight]},\n",
    "    {'params': [C_network[4].weight]},\n",
    "    {'params': [C_network[6].weight]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].weight]},\n",
    "    {'params': [N_network[2].weight]},\n",
    "    {'params': [N_network[4].weight]},\n",
    "    {'params': [N_network[6].weight]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].weight]},\n",
    "    {'params': [O_network[2].weight]},\n",
    "    {'params': [O_network[4].weight]},\n",
    "    {'params': [O_network[6].weight]},\n",
    "])\n",
    "\n",
    "SGD = torch.optim.SGD([\n",
    "    # H networks\n",
    "    {'params': [H_network[0].bias]},\n",
    "    {'params': [H_network[2].bias]},\n",
    "    {'params': [H_network[4].bias]},\n",
    "    {'params': [H_network[6].bias]},\n",
    "    # C networks\n",
    "    {'params': [C_network[0].bias]},\n",
    "    {'params': [C_network[2].bias]},\n",
    "    {'params': [C_network[4].bias]},\n",
    "    {'params': [C_network[6].bias]},\n",
    "    # N networks\n",
    "    {'params': [N_network[0].bias]},\n",
    "    {'params': [N_network[2].bias]},\n",
    "    {'params': [N_network[4].bias]},\n",
    "    {'params': [N_network[6].bias]},\n",
    "    # O networks\n",
    "    {'params': [O_network[0].bias]},\n",
    "    {'params': [O_network[2].bias]},\n",
    "    {'params': [O_network[4].bias]},\n",
    "    {'params': [O_network[6].bias]},\n",
    "], lr=1e-3)\n",
    "\n",
    "AdamW_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(AdamW, factor=0.5, patience=100, threshold=0)\n",
    "SGD_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(SGD, factor=0.5, patience=100, threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a training checkpoint file saved, we can save the name of the file to `latest_checkpoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = 'torch-checkpoint-files/Training-RTP-EFH-latest.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume training from previously saved checkpoints:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(latest_checkpoint):\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    nn.load_state_dict(checkpoint['nn'])\n",
    "    AdamW.load_state_dict(checkpoint['AdamW'])\n",
    "    SGD.load_state_dict(checkpoint['SGD'])\n",
    "    AdamW_scheduler.load_state_dict(checkpoint['AdamW_scheduler'])\n",
    "    SGD_scheduler.load_state_dict(checkpoint['SGD_scheduler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we need to validate on validation set and if validation error\n",
    "is better than the best, then save the new best model to a checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    # run validation\n",
    "    mse_sum = torch.nn.MSELoss(reduction='sum')\n",
    "    energy_mse = 0.0\n",
    "    force_mse = 0.0\n",
    "    hessian_mse = 0.0\n",
    "    n_molec = 0\n",
    "    n_force_elements = 0\n",
    "    n_Hessian_elements = 0\n",
    "    \n",
    "    for properties in validation:\n",
    "        # Save the properties in variables\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).requires_grad_(True)\n",
    "        true_energies = properties['energies'].to(device)\n",
    "        true_forces = properties['forces'].to(device)\n",
    "        true_hessian = properties['hessian'].to(device)\n",
    "        \n",
    "        # Predict energies from our model\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "        \n",
    "        # Calculate the predicted forces and the Hessian from our model\n",
    "        forces = -torch.autograd.grad(predicted_energies.sum(), coordinates, create_graph=True, retain_graph=True)[0]\n",
    "        hessian = torchani.utils.hessian(coordinates, forces=forces, retain_graph=True)\n",
    "        \n",
    "        # Calculate the number of elements in the energies, forces, and Hessian tensors\n",
    "        n_molec += predicted_energies.shape[0] # The number of molecules is equal to the number of elements in the energy tensor\n",
    "        #print(\"Number of molecules:\", n_molec)\n",
    "        N = (species >= 0).sum(dim=1, dtype=true_energies.dtype) # N is a tensor with the number of atoms in each molecule\n",
    "        n_force_elements += sum(3*N).item() # Total number of force elements in the batch\n",
    "        n_Hessian_elements += sum(9*N**2).item() # Total number of Hessian elements in the batch\n",
    "        \n",
    "        # Sum the Mean Squared Errors from the energies, forces and Hessian tensors\n",
    "        energy_mse += mse_sum(predicted_energies, true_energies).item()\n",
    "        force_mse += mse_sum(forces, true_forces).item()\n",
    "        hessian_mse += mse_sum(hessian, true_hessian).item()\n",
    "        \n",
    "    # Return the RMSE\n",
    "    return hartree2kcalmol(math.sqrt(energy_mse / n_molec)), \\\n",
    "           hartree2kcalmol(math.sqrt(force_mse / n_force_elements)), \\\n",
    "           hartree2kcalmol(math.sqrt(hessian_mse / n_Hessian_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rmse():\n",
    "    # run validation\n",
    "    mse_sum = torch.nn.MSELoss(reduction='sum')\n",
    "    energy_mse = 0.0\n",
    "    force_mse = 0.0\n",
    "    hessian_mse = 0.0\n",
    "    n_molec = 0\n",
    "    n_force_elements = 0\n",
    "    n_Hessian_elements = 0\n",
    "    \n",
    "    for properties in test:\n",
    "        # Save the properties in variables\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).requires_grad_(True)\n",
    "        true_energies = properties['energies'].to(device)\n",
    "        true_forces = properties['forces'].to(device)\n",
    "        true_hessian = properties['hessian'].to(device)\n",
    "        \n",
    "        # Predict energies from our model\n",
    "        _, predicted_energies = model((species, coordinates))\n",
    "        \n",
    "        # Calculate the predicted forces and the Hessian from our model\n",
    "        forces = -torch.autograd.grad(predicted_energies.sum(), coordinates, create_graph=True, retain_graph=True)[0]\n",
    "        hessian = torchani.utils.hessian(coordinates, forces=forces, retain_graph=True)\n",
    "        \n",
    "        # Calculate the number of elements in the energies, forces, and Hessian tensors\n",
    "        n_molec += predicted_energies.shape[0] # The number of molecules is equal to the number of elements in the energy tensor\n",
    "        #print(\"Number of molecules:\", n_molec)\n",
    "        N = (species >= 0).sum(dim=1, dtype=true_energies.dtype) # N is a tensor with the number of atoms in each molecule\n",
    "        n_force_elements += sum(3*N).item() # Total number of force elements in the batch\n",
    "        n_Hessian_elements += sum(9*N**2).item() # Total number of Hessian elements in the batch\n",
    "        \n",
    "        # Sum the Mean Squared Errors from the energies, forces and Hessian tensors\n",
    "        energy_mse += mse_sum(predicted_energies, true_energies).item()\n",
    "        force_mse += mse_sum(forces, true_forces).item()\n",
    "        hessian_mse += mse_sum(hessian, true_hessian).item()\n",
    "        \n",
    "    # Return the RMSE\n",
    "    return hartree2kcalmol(math.sqrt(energy_mse / n_molec)), \\\n",
    "           hartree2kcalmol(math.sqrt(force_mse / n_force_elements)), \\\n",
    "           hartree2kcalmol(math.sqrt(hessian_mse / n_Hessian_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop, we need to compute force, Hessian, loss for forces, and loss for Hessian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional** \n",
    "\n",
    "Note that forces and Hessian are also computed in the cell below, and the loss is calculated by considering energy error, force error, and Hessian error.\n",
    "\n",
    "`force_coefficient` and `hessian_coefficient` sets the importance of the forces and the Hessian w.r.t. energies, respectively.\n",
    "\n",
    "For more details on how different properties are calculated, see `gao2020`, especially Table 1 and example 3/Listing 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "print(\"training starting from epoch\", AdamW_scheduler.last_epoch + 1)\n",
    "# We only train 3 epoches here in able to generate the docs quickly.\n",
    "# Real training should take much more than 3 epoches.\n",
    "max_epochs = 3  # or try: config['max_epochs']\n",
    "early_stopping_learning_rate = 1.0E-5\n",
    "force_coefficient = 0.08  # controls the importance of energy loss vs force loss\n",
    "hessian_coefficient = 0.02  # controls the importance of energy loss vs Hessian loss\n",
    "best_model_checkpoint = 'torch-checkpoint-files/Training-RTP-EFH-best.pt'\n",
    "\n",
    "# Initialize wandb to keep track of the training\n",
    "wandb.init(project=\"project name\", entity=\"user\", config=config, tags=[\"tag 1\",\"tag 2\"],\n",
    "        name='EFH-training') # fill each kwarg with your custom run information\n",
    "# (id=\"run_id\", resume=\"must\") for resuming a previous run. Replace \"rund_id\" for yout wandb run ID\n",
    "\n",
    "for _ in range(AdamW_scheduler.last_epoch + 1, max_epochs):\n",
    "    \n",
    "    \n",
    "    e_rmse, fc_rmse, hess_rmse = validate()\n",
    "\n",
    "    print('Energy RMSE:', e_rmse, 'Force RMSE:', fc_rmse, 'and Hessian RMSE:', hess_rmse, 'at epoch', AdamW_scheduler.last_epoch + 1)\n",
    "\n",
    "    learning_rate = AdamW.param_groups[0]['lr']\n",
    "\n",
    "    if learning_rate < early_stopping_learning_rate:\n",
    "        break\n",
    "\n",
    "    # checkpoint\n",
    "    if AdamW_scheduler.is_better(e_rmse, AdamW_scheduler.best):\n",
    "        torch.save(nn.state_dict(), best_model_checkpoint)\n",
    "\n",
    "    AdamW_scheduler.step(e_rmse)\n",
    "    SGD_scheduler.step(e_rmse)\n",
    "    \n",
    "    wandb.log({'validation_energy_rmse':e_rmse, 'validation_forces_rmse':fc_rmse, 'validation_hessian_rmse':hess_rmse,\n",
    "               'best_validation_energy_rmse': AdamW_scheduler.best, \n",
    "               'learning_rate': learning_rate}, \n",
    "              step = AdamW_scheduler.last_epoch + 1)\n",
    "    \n",
    "    e_rmse, fc_rmse, hess_rmse = test_rmse()\n",
    "    wandb.log({'test_energy_rmse':e_rmse, 'test_forces_rmse':fc_rmse, 'test_hessian_rmse':hess_rmse}, \n",
    "              step = AdamW_scheduler.last_epoch + 1)\n",
    "    \n",
    "    # Besides being stored in x, species and coordinates are also stored in y.\n",
    "    # So here, for simplicity, we just ignore the x and use y for everything.\n",
    "    for i, properties in tqdm.tqdm(\n",
    "        enumerate(training),\n",
    "        total=len(training),\n",
    "        desc=\"epoch {}\".format(AdamW_scheduler.last_epoch)\n",
    "    ):\n",
    "        species = properties['species'].to(device)\n",
    "        coordinates = properties['coordinates'].to(device).requires_grad_(True)\n",
    "        true_energies = properties['energies'].to(device)\n",
    "        true_forces = properties['forces'].to(device)\n",
    "        true_hessian = properties['hessian'].to(device)\n",
    "        num_atoms = (species >= 0).sum(dim=1, dtype=true_energies.dtype)\n",
    "        _, predicted_energies = model((species, coordinates)) # 1 forward pass\n",
    "\n",
    "        # We can use torch.autograd.grad to compute force and Hessian. \n",
    "        # Remember to create graph so that the loss of the force and \n",
    "        # the loss of the Hessian can contribute to the gradient of \n",
    "        # parameters, and also to retain graph so that we can backward\n",
    "        # through it a second time when computing gradient w.r.t.\n",
    "        # parameters.\n",
    "        \n",
    "        # Calculate the atomic forces\n",
    "        forces = -torch.autograd.grad(predicted_energies.sum(), coordinates, create_graph=True, retain_graph=True)[0]\n",
    "        # 1 backward pass\n",
    "        \n",
    "        # Calculate the Hessian\n",
    "        hessian = torchani.utils.hessian(coordinates, forces=forces, retain_graph=True, create_graph=True)\n",
    "        # 3N backward passes\n",
    "        \n",
    "        # Now the total loss has three parts, energy loss, force loss, and Hessian loss\n",
    "        energy_loss = (mse(predicted_energies, true_energies) / num_atoms.sqrt()).mean()\n",
    "        force_loss = (mse(true_forces, forces).sum(dim=(1, 2)) / (3*num_atoms)).mean()\n",
    "        hessian_loss = (mse(true_hessian, hessian).sum(dim=(1, 2)) / (9*num_atoms**2)).mean()\n",
    "\n",
    "        # Loss function\n",
    "        loss = energy_loss + force_coefficient * force_loss + hessian_coefficient * hessian_loss \n",
    "        # 1 backward pass\n",
    "        \n",
    "        AdamW.zero_grad()\n",
    "        SGD.zero_grad()\n",
    "        loss.backward()\n",
    "        AdamW.step()\n",
    "        SGD.step()\n",
    "        \n",
    "        # Write current batch loss to WandB\n",
    "        #wandb.log({'Total loss': loss,'Energy loss': energy_loss, 'Force loss': force_loss, 'Hessian loss': hessian_loss})\n",
    "    \n",
    "    # Save the information into a checkpoint file\n",
    "    torch.save({\n",
    "        'nn': nn.state_dict(),\n",
    "        'AdamW': AdamW.state_dict(),\n",
    "        'SGD': SGD.state_dict(),\n",
    "        'AdamW_scheduler': AdamW_scheduler.state_dict(),\n",
    "        'SGD_scheduler': SGD_scheduler.state_dict(),\n",
    "    }, latest_checkpoint)\n",
    "    \n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
